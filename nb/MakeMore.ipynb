{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "words = open(\"../names.txt\", \"r\").read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bigram: refers to only 2 characters\n",
    "- bigram based NN: takes the first character to predict the next character (two characters only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "\n",
    "for word in words:\n",
    "    # add beginning and end token\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        # count the numbers of unique bigrams that occur\n",
    "        # and store them in a dict\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "        \n",
    "# sort by the number of occurring bigrams\n",
    "sorted_b = sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict to map the character with the position/idx in 2d array\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {ch:i +1 for i, ch in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "\n",
    "# create array to store the bigrams\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "for word in words:\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        N[idx1, idx2] += 1\n",
    "\n",
    "# reverse the idx:ch dict\n",
    "itos = {b:a for a, b in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telicalanaa.\n",
      "ses.\n",
      "dyaraistonainunda.\n",
      "pellio.\n",
      "tele.\n",
      "mandinnliexaiali.\n",
      "rlyizermian.\n",
      "kheon.\n",
      "garai.\n",
      "asar.\n"
     ]
    }
   ],
   "source": [
    "P = N.float()\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "g = torch.Generator().manual_seed(121241)\n",
    "\n",
    "for i in range(10): # create 10 names\n",
    "    name = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        ch = itos[ix] \n",
    "        name.append(ch)\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(\"\".join(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".t -3.1982674598693848\n",
      "tu -4.268441677093506\n",
      "un -2.4336133003234863\n",
      "ng -4.206658840179443\n",
      "g. -2.8815884590148926\n",
      "tensor(2.8816)\n"
     ]
    }
   ],
   "source": [
    "agg_prob = 0\n",
    "for word in [\"tung\"]:\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "\n",
    "        p = P[ix1, ix2]\n",
    "        agg_prob =+ -torch.log(p)\n",
    "        print(f\"{ch1}{ch2} {torch.log(p)}\")\n",
    "\n",
    "    print(agg_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train and test dataset\n",
    "xs, ys = [], []\n",
    "for word in words[:1]:\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "# encode xs and ys\n",
    "x_enc = F.one_hot(xs, num_classes=27).float()\n",
    "y_enc = F.one_hot(ys, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27))\n",
    "\n",
    "logits = x_enc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0149, 0.0154, 0.0646, 0.0288, 0.0021, 0.0293, 0.0083, 0.0071, 0.0151,\n",
       "         0.0368, 0.0317, 0.0095, 0.0135, 0.0768, 0.0172, 0.0203, 0.0101, 0.0177,\n",
       "         0.0872, 0.1742, 0.0206, 0.0077, 0.0040, 0.1868, 0.0527, 0.0405, 0.0071],\n",
       "        [0.0564, 0.0615, 0.0528, 0.0242, 0.0069, 0.0537, 0.0838, 0.0096, 0.0556,\n",
       "         0.0205, 0.0136, 0.0018, 0.0124, 0.0972, 0.0321, 0.0398, 0.0664, 0.0175,\n",
       "         0.0475, 0.0160, 0.0735, 0.0803, 0.0255, 0.0070, 0.0070, 0.0135, 0.0239],\n",
       "        [0.0637, 0.0046, 0.0070, 0.0102, 0.0151, 0.0445, 0.0316, 0.0046, 0.0268,\n",
       "         0.0231, 0.0708, 0.0053, 0.0268, 0.1540, 0.2158, 0.0132, 0.0362, 0.0047,\n",
       "         0.0562, 0.0360, 0.0538, 0.0320, 0.0063, 0.0276, 0.0104, 0.0073, 0.0126],\n",
       "        [0.0637, 0.0046, 0.0070, 0.0102, 0.0151, 0.0445, 0.0316, 0.0046, 0.0268,\n",
       "         0.0231, 0.0708, 0.0053, 0.0268, 0.1540, 0.2158, 0.0132, 0.0362, 0.0047,\n",
       "         0.0562, 0.0360, 0.0538, 0.0320, 0.0063, 0.0276, 0.0104, 0.0073, 0.0126],\n",
       "        [0.0391, 0.0125, 0.2476, 0.0480, 0.0384, 0.0157, 0.0554, 0.0543, 0.0697,\n",
       "         0.0086, 0.0210, 0.0385, 0.0387, 0.1140, 0.0154, 0.0052, 0.0090, 0.0257,\n",
       "         0.0155, 0.0254, 0.0116, 0.0118, 0.0105, 0.0155, 0.0174, 0.0235, 0.0119]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "select_lfq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
